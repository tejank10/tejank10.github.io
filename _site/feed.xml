<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-06-11T02:22:32+05:30</updated><id>http://localhost:4000/</id><title type="html">Tejan Karmali</title><subtitle>Here, I share my experience during GSoC and Deep learning.</subtitle><entry><title type="html">GSoC’18: From Go to AlphaGo</title><link href="http://localhost:4000/jekyll/update/2018/06/09/GSoC-week3-4.html" rel="alternate" type="text/html" title="GSoC'18: From Go to AlphaGo" /><published>2018-06-09T16:00:00+05:30</published><updated>2018-06-09T16:00:00+05:30</updated><id>http://localhost:4000/jekyll/update/2018/06/09/GSoC-week3-4</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/06/09/GSoC-week3-4.html">&lt;p&gt;Hello, world!&lt;/p&gt;

&lt;p&gt;In the last post, I had talked about the game of Go and its dynamics. In last two weeks, I worked on the set objectives which were Monte-Carlo Tree Search and putting things together to make AlphaGo Zero.&lt;/p&gt;

&lt;p&gt;In the week 3, it was mostly about MCTS. The MCTS is organized into two parts. One is a &lt;code class=&quot;highlighter-rouge&quot;&gt;struct&lt;/code&gt; for a node of tree. The other is a &lt;code class=&quot;highlighter-rouge&quot;&gt;struct&lt;/code&gt; for Player which uses MCTS to perform move. &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt; is the &lt;code class=&quot;highlighter-rouge&quot;&gt;struct&lt;/code&gt; with which the user will interact. A node of Monte-Carlo tree is defined by a board position, and the different positions it can go to upon playing any of the moves from that board position in the action space. &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt; provides an API to perform Tree Search. It then selects a move based on tree search, and perform it. While performing tree search virtual loss was used. It means that when selecting a node, it is pretended that evaluation has already taken place. This introduces some stochasticity in selection of node.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt; consists of a neural network. This neural network is used to generate a prediction of policy and value for a given input of board position. This prediction is used to update the values related to the nodes (which we had earlier used as virtual loss). &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt;’s neural network is of type &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt;, which is broken down into 3 parts: Base network, value head and policy head. The input passes through the base network first. The output of it is fed into value head to obtain value of state and into policy head to obtain the policy for it. &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt; can also perform the evaluation of two &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt;s, where two players compete in a series of games to decide who is the winner.&lt;/p&gt;

&lt;p&gt;I have put up an example of AlphaGo Zero algorithm &lt;a href=&quot;https://github.com/tejank10/AlphaGo.jl/blob/master/src/play.jl&quot;&gt;here&lt;/a&gt;. By setting the flags, you can run it. By default it runs the default AGZ algorithms from the paper.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hello, world!</summary></entry><entry><title type="html">GSoC’18: Flux baselines, Go and more</title><link href="http://localhost:4000/jekyll/update/2018/05/26/GSoC-week1-2.html" rel="alternate" type="text/html" title="GSoC'18: Flux baselines, Go and more" /><published>2018-05-26T16:00:00+05:30</published><updated>2018-05-26T16:00:00+05:30</updated><id>http://localhost:4000/jekyll/update/2018/05/26/GSoC-week1-2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/26/GSoC-week1-2.html">&lt;p&gt;Hello, world!&lt;/p&gt;

&lt;p&gt;The commmunity bonding period and first 2 weeks of GSoC has come to an end. Community bonding period lasted over three weeks. I coult not do much work over first two weeks due to my end semester exams. In the third week, I implemented &lt;a href=&quot;https://arxiv.org/abs/1502.03509&quot;&gt;MADE&lt;/a&gt;, which is Masked Autoencoder for Distributed Estimation.  I also added dilation feature for convolutions (which was a &lt;a href=&quot;https://github.com/FluxML/NNlib.jl/pull/31#issuecomment-386673632&quot;&gt;feature request&lt;/a&gt; for NNlib.jl).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/FluxML/model-zoo/pull/39&quot;&gt;PR#19&lt;/a&gt;: Implemented MADE architecture in Flux.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/FluxML/NNlib.jl/pull/40&quot;&gt;PR#40&lt;/a&gt;: Dilation support for convolutions. So far, dilation support is available for 1D, 2D and 3D convolutions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In week 1, I implemented and created demos of some seminal papers in deep reinforcement learning. The algorithms implemented were tested on environments in OpenAI Gym using the package &lt;a href=&quot;https://github.com/JuliaML/OpenAIGym.jl&quot;&gt;OpenAIGym.jl&lt;/a&gt;. Environments used for testing are CartPole-v0, Pong-v0 and Pendulum-v0. The work done in this regard over pre-GSoC period and this week has been compiled into &lt;a href=&quot;https://github.com/tejank10/Flux-baselines&quot;&gt;Flux baselines&lt;/a&gt; repo. As of now it contains 6 models, which include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/dqn.jl&quot;&gt;Deep Q Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/double-dqn.jl&quot;&gt;Double DQN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/prioritized-replay-dqn.jl&quot;&gt;Prioritized Experience Replay DQN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/duel-dqn.jl&quot;&gt;Dueling DDQN &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/actor-critic/a2c.jl&quot;&gt;Advantage Actor-Critic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/ddpg/ddpg.jl&quot;&gt;Deep Deterministic Policy Gradients&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In week 2, I started working towards one major milestone of the project: the &lt;a href=&quot;https://deepmind.com/blog/alphago-zero-learning-scratch/&quot;&gt;AlphaGo Zero&lt;/a&gt; model. For those of you not familiar, AlphaGo Zero is latest version of AlphaGo which is a program to play (and defeat :P) the ancient Chinese game of Go. This version of AlphaGo doesn’t take any human amateur and professional games to learn how to play Go. Instead it learns to play by playing games against itself, starting from completely random play.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tejank10/AlphaGo.jl&quot;&gt;AlphaGo.jl&lt;/a&gt; is where I am implementing the Flux based version of AlphaGo Zero. This part of project is divided  into three tasks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Creating the environment for Go&lt;/li&gt;
  &lt;li&gt;Monte-Carlo Tree Search&lt;/li&gt;
  &lt;li&gt;Main model of AlphaGo Zero using Go and MCTS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In week 2 I created the environment of Go. The environment simulates the game of Go, with abstraction like that of OpenAI Gym. The game can be played on a board of size 9x9, 13x13, 17x17 or 19x19. A player is assigned stones of either black or white color. The player with black stones makes the first move. Now, this can be an advantage for the black player. Hence white player is awarded some extra points. These extra points are called komi. Players can place a stone on any intersection of a vertical and horizontal lines on the board. A &lt;em&gt;NxN&lt;/em&gt; board has &lt;em&gt;N^2&lt;/em&gt; intersections.  On a player’s turn, he can either place a stone or can pass to the other player. Thus, action space for the environment is &lt;em&gt;N^2 + 1&lt;/em&gt;. The game ends when both the players pass consecutively. Depending on the end game state of the board, scores are calculated and winner is decided.&lt;/p&gt;

&lt;p&gt;In the coming days, my goals will be to complete the other two tasks. Hopefully in the next blog post, I’ll present the demo of the game before you.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hello, world!</summary></entry><entry><title type="html">Accepted to GSoC’18</title><link href="http://localhost:4000/jekyll/update/2018/05/04/Accepted-to-GSoC.html" rel="alternate" type="text/html" title="Accepted to GSoC'18" /><published>2018-05-04T16:00:00+05:30</published><updated>2018-05-04T16:00:00+05:30</updated><id>http://localhost:4000/jekyll/update/2018/05/04/Accepted-to-GSoC</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/04/Accepted-to-GSoC.html">&lt;p&gt;Hello, world!&lt;/p&gt;

&lt;p&gt;I am Tejan Karmali, a CS undergrad at NIT Goa. My proposal for GSoC’18 has been &lt;a href=&quot;https://summerofcode.withgoogle.com/projects/#5885584111828992&quot;&gt;accepted&lt;/a&gt; under NumFOCUS: Julia. I’ll be working on enriching the model zoo of &lt;a href=&quot;http://fluxml.ai/&quot;&gt;Flux.jl&lt;/a&gt;. Flux is a Machine Learning library written fully in Julia. My project aims at adding a variety of models in Flux. I’ll be adding Dueling DQN, Actor-Critic model, AlphaGo, DCGAN, Decoupling Neural Interface and Spatial Transformer Networks in the model zoo over the course of summer. These models are targeted towards the new users of Flux and to help them get started.&lt;/p&gt;

&lt;p&gt;My mentors are &lt;a href=&quot;https://mikeinnes.github.io/&quot;&gt;Mike Innes&lt;/a&gt;, &lt;a href=&quot;https://simondanisch.jimdo.com/&quot;&gt;Simon Danisch&lt;/a&gt;, &lt;a href=&quot;http://chrisrackauckas.com/&quot;&gt;Chris Rackauckas&lt;/a&gt; and &lt;a href=&quot;http://karpinski.org/&quot;&gt;Stefan Karpinski&lt;/a&gt;. I’ll be posting biweekly updates here about my project. Looking forward to an eventful and productive summer!&lt;/p&gt;</content><author><name></name></author><summary type="html">Hello, world!</summary></entry></feed>