<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-08-25T22:07:25+05:30</updated><id>http://localhost:4000/</id><title type="html">Tejan Karmali</title><subtitle>Here, I share my experience during GSoC and Deep learning.</subtitle><entry><title type="html">GSoC’19: Differentiable Duckietown</title><link href="http://localhost:4000/jekyll/update/2018/08/19/GSoC-2019-Differentiable-Duckietown.html" rel="alternate" type="text/html" title="GSoC'19: Differentiable Duckietown" /><published>2018-08-19T16:00:00+05:30</published><updated>2018-08-19T16:00:00+05:30</updated><id>http://localhost:4000/jekyll/update/2018/08/19/GSoC-2019-Differentiable-Duckietown</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/08/19/GSoC-2019-Differentiable-Duckietown.html">&lt;p&gt;Hello there,&lt;/p&gt;

&lt;p&gt;Over the past year I continued my streak with Julia by contributing to &lt;a href=&quot;https://fluxml.ai/2019/03/05/dp-vs-rl.html&quot;&gt;some interesting experiments&lt;/a&gt; with &lt;a href=&quot;https://fluxml.ai/2019/02/07/what-is-differentiable-programming.html&quot;&gt;differentiable programming&lt;/a&gt;. That got me super-excited about the paradigm of differentiable learning. Main idea being that if we have knowledge about the system, it could be used to simplify and accelerate the training process. Together with Mike and Avik, we planned on a mission: a self driving car simulator using differentiable programming.&lt;/p&gt;

&lt;p&gt;We chose duckietown environment to test our approach. &lt;a href=&quot;https://www,duckietown.org&quot;&gt;Duckietown&lt;/a&gt; is a project started by Prof. Liam Paull. It is miniature model of a town having buildings, vehicles, traffic signals, and pedestrians. Maxime Chevalier-Boisvert et al, from MILA have built an awesome simulator of the duckietown, called &lt;a href=&quot;https://github.com/duckietown/gym-duckietown&quot;&gt;gym-duckietown&lt;/a&gt;. It is used for testing algorithms before deploying it in real duckietown environment. But since it is written using python, it is not differentiable. Hence to make it differentiable we had to build it in julia.&lt;/p&gt;

&lt;p&gt;Duckietown.jl creation is spread over three parts:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The Simulator&lt;/li&gt;
  &lt;li&gt;Rendering with RayTracer&lt;/li&gt;
  &lt;li&gt;Training&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The project can be found &lt;a href=&quot;https;//github.com/tejank10/Duckietown.jl&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let’s get started!&lt;/p&gt;

&lt;h2 id=&quot;the-environment&quot;&gt;The Environment&lt;/h2&gt;
&lt;p&gt;Duckietown environment comtains maps for different tasks - straight road, loop, zigzag turns, UdeM etc. There are also some variants of these maps with dynamic objects, like traffic signal and pedestrians. The maps are encoded in a .yaml files and can be parsed with &lt;a href=&quot;https://github.com/BioJulia/YAML.jl&quot;&gt;YAML.jl&lt;/a&gt;. Each environments contains a map in the form of a grid. Each element of the grid is allocated a tile: for eg. a road, an asphault, an office floor or it can be a grassy surface. A logical arrangement of these tiles is all that is required for a bare bones version of map to set up. That’s all the straight road and loopy road maps have. To make these maps more challenging, we need to add objects to it. Objects can be static or dynamic. Static objects include house, tree, triffic sign, bus, truck, traffic cone etc. Dynamic objects are traffic signals, and duckies which are the pedestrians in the duckietown. The positions of these objects are defined in the map. An object is represented as meshes, with texture wrapped around it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/tejank10/tejank10.github.io/raw/master/assets/udem1.gif&quot; alt=&quot;UdeM&quot; align=&quot;middle&quot; width=&quot;400&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-simulator&quot;&gt;The Simulator&lt;/h2&gt;
&lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Duckietown&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flux&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Zygote&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Simulator&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;straight_road&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;camera_width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;camera_height&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Simulator manages the subtasks involved in running the duckietown and
maintains related statistics. The subtasks include updating the states and positions of different objects involved, running an action on duckiebot, maintaining data such as velocity, position, action performed on the duckiebot, rendering what the bit sees etc. The parameters of the simulator are defined in a &lt;code class=&quot;highlighter-rouge&quot;&gt;FixedParams&lt;/code&gt; objects. These are the parameters that define the behaviour of the simulator.&lt;/p&gt;

&lt;h2 id=&quot;rendering-the-view&quot;&gt;Rendering the view&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rende_obs(...)&lt;/code&gt; is used to render was duckiebot sees. We use differentiable &lt;a href=&quot;https://github.com/avik-pal/RayTracer.jl&quot;&gt;RayTracer.jl&lt;/a&gt; for this purpose. For rendering we first need to define a camera model. Camera needs to know where the bot is looking from &amp;amp; at, dimensions of image, fied of view, focal length, and up vector.&lt;/p&gt;
&lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur_pos&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# get the direction in which bot is looking&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_dir_vec&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur_angle&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Define camera model&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Looking from&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vec3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Looking at&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vec3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dz&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# vup is vector pointing in upward direction&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vec3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0f0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1f0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0f0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Camera&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vup&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cam_fov_y&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;focal_length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cam_width&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cam_height&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;A scene is generated containing all the objects in the environments. These objects are decomposed into triangles.&lt;/p&gt;
&lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Scene generation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scene&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vector&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Triangle&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;}()&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Decompose the objects into triangles&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;obj_Δs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;draw_bbox&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;objs&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oΔ&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;obj_Δs&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scene&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vcat&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scene&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;oΔ&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Light source and its position is defined, which is then used to raytrace the scene.&lt;/p&gt;
&lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Define light source&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;light_pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vec3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;40f0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;200f0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;100f0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# PointLight takes color, intensity and position of light source as args&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;light&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PointLight&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vec3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1f0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5f15&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;light_pos&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;direction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_primary_rays&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cam&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Rendering what duckiebot sees&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raytrace&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;direction&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;observation&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;light&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;origin&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;taking-action&quot;&gt;Taking action&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;step!(...)&lt;/code&gt; is used to take action on the duckiebot. action is a vector of length 2. It specifies
speed of left and right wheel. each element belongs to [-1, 1], where positive speed implies moving in the forward direction. Velocities of both the wheels give us the information about the steering direction of the robot. For eg: to move on a straight road both the velocities should be equal, whereas to take a left turn velocity of left wheel should be less than that of right wheel. Using this, robot’s new position and direciton is calculated.&lt;/p&gt;

&lt;p&gt;A path to be followed is determined by bezier curves. Each tile has its own bezier curve defined. For example, a straight road tile would have a straight line as its curve whereas that for a left turn would be approximately circular. Based on this curve, two kinds of rewards are defined. The distance from the curve and angular distance from the tangent of the curve. There is also a penalty to prevent collision. Each object has a safety circle surrounding itself. Collision penalty is the degree of overlap between the the safety circle of the bot and that of an object.&lt;/p&gt;

&lt;p&gt;With these details, we are now equipped to train a model!&lt;/p&gt;

&lt;h2 id=&quot;training-a-model&quot;&gt;Training a model&lt;/h2&gt;
&lt;p&gt;We define a simple Flux model, which extracts features from the image using &lt;code class=&quot;highlighter-rouge&quot;&gt;Conv&lt;/code&gt; and passes them onto the FC layers.&lt;/p&gt;
&lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# model: Input- Rendering of what duckiebot sees&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#        Output- Action to be taken&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Chain&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;Conv&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;MaxPool&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)),&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;Conv&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;MaxPool&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)),&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;Conv&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ADAM&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001f0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We begin with dividing an episode into sequences. Let’s call a sequence as &lt;code class=&quot;highlighter-rouge&quot;&gt;μEpisode&lt;/code&gt;. In each &lt;code class=&quot;highlighter-rouge&quot;&gt;μEpisode&lt;/code&gt;, actions are perormed for short number of timesteps. We take loss as negative of reward and add an action penalty. Recall that actions should lie in [-1, 1]. Since for initial few timesteps actions could arbirtrarily lie anywhere in the real domain, this penalty is required. Also, reward is proportional to speed. If Very high action is chosen, than it sould also set very high speed and in turn very high reward, which is not expected ideally. Action penalty is somewhat similar to the regularisation loss.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function μEpisode(model, sim, initial_render, μEp_len)
    obs, action, reward, done, info = step!(sim, model(initial_render))
    loss = -reward + action_penalty(action)

    done &amp;amp;&amp;amp; return loss

    for iter in 2:μEp_len
        obs, action, reward, done, info = step!(sim, model(obs))
        loss += -reward + action_penalty(action)
        done &amp;amp;&amp;amp; return loss
    end

    return loss
end
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In the &lt;code class=&quot;highlighter-rouge&quot;&gt;episode!(...)&lt;/code&gt; function, gradient of the loss wrt to the parameters of the &lt;code class=&quot;highlighter-rouge&quot;&gt;model&lt;/code&gt; is taken using &lt;a href=&quot;https://github.com/FluxML/Zygote.jl&quot;&gt;Zygote.jl&lt;/a&gt;, a source-to-source AD package. Gradients are clamped in order ot prevent the overflow due to gradient explosion.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function episode!(sim)    
    for μEp in 1:NUM_μEPISODES
        # Get the gradients of μEpisode
        initial_render = render_obs(sim)
        gs = Zygote.gradient(params(model)) do
            μEpisode(model, sim, initial_render, μEPISODE_LENGTH)
        end

        # Update the weights
        for p in params(model)
            clamp!(gs[p], -0.01f0, 0.01f0)
            Flux.Optimise.update!(opt, p, gs[p])
        end

        sim.done &amp;amp;&amp;amp; break
    end

    reset!(sim)
end
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And after a while, you should be able to see the bot guiding itself on the lane!
&lt;img src=&quot;https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/straight_road_w_text.gif&quot; alt=&quot;Straight road&quot; align=&quot;middle&quot; width=&quot;400&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;What a productive summer it was!. I believe this is just a start for differentiable programming. By having knowledge about the system, we can speed up the training of a model on it by leaps and bounds. In future, I plan to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Transfer learning: Evaluating the performace model trained on one map by testing it on other maps.&lt;/li&gt;
  &lt;li&gt;Defining tasks over different maps&lt;/li&gt;
  &lt;li&gt;There has been some advances in terms of the &lt;a href=&quot;https://phyre.ai&quot;&gt;pakcages&lt;/a&gt; for physical environements for deep learning. I plan to do some experiments on that using diffferentiable programming.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ackowledgements&quot;&gt;Ackowledgements&lt;/h2&gt;
&lt;p&gt;I am extremely grateful to my mentor Mike Innes for posing faith in me for this ambitious project. A huge thanks to my fellow GSoC’er Avik Pal for his amazing RayTracer, and helping me out from time to time. I would also like to thank Dhairya Gandhi for his valuable inputs, Julia Computing Bengaluru for hosting me, and Julia Computing for providing machines for training. Finally, I thank Google for providing me this amazing opportunity in being part of the mission to drive open source culture.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hello there,</summary></entry><entry><title type="html">GSoC’18: Final Summary</title><link href="http://localhost:4000/jekyll/update/2018/08/06/GSoC-Final-Summary.html" rel="alternate" type="text/html" title="GSoC'18: Final Summary" /><published>2018-08-06T16:00:00+05:30</published><updated>2018-08-06T16:00:00+05:30</updated><id>http://localhost:4000/jekyll/update/2018/08/06/GSoC-Final-Summary</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/08/06/GSoC-Final-Summary.html">&lt;p&gt;Hello, world!&lt;/p&gt;

&lt;p&gt;In this post I’m going to briefly summarize about the machine learning models I have worked on during this summer for GSoC. I worked towards enriching model zoo of &lt;a href=&quot;https://github.com/FluxML&quot;&gt;Flux.jl&lt;/a&gt;, a machine learning library written in &lt;a href=&quot;https://github.com/julia/julialang&quot;&gt;Julia&lt;/a&gt;. My project covered Reinforcement Learning and computer vision models.&lt;/p&gt;

&lt;p&gt;The project is spread over these 4 codebases&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines&quot;&gt;Flux-baselines&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/AlphaGo.jl&quot;&gt;AlphaGo.jl&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/model-zoo/tree/GAN&quot;&gt;GAN models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/model-zoo/tree/DNI&quot;&gt;DNI model&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the process, I could achieve most of my targets. I had to skip a few of them, and also made some unplanned models. Below, I discuss about these issues repository wise.&lt;/p&gt;

&lt;h3 id=&quot;1-flux-baselines&quot;&gt;1. &lt;a href=&quot;https://github.com/tejank10/Flux-baselines&quot;&gt;Flux-baselines&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Flux baselines is a collection of various Deep Reinforcement Learning models. This includes Deep Q Networks, Actor-Critic and DDPG.&lt;/p&gt;

&lt;p&gt;Basic structure of an RL probem is as folowd: There is an environment, let’s say game of pong is our environment. The environment may contain many ojects which interact with each other. In pong there are 3 objects: a ball and 2 paddles. The environment has a &lt;em&gt;state&lt;/em&gt;. It is the current situation present in the environment in terms of various features of the objects in it. These features could be position, velocity, color etc. pertaining to the objects in the it. An actions needs to be chosed to play a move in the environment and obtain the next state. Actions will be chosen till the game ends. An RL model basically finds the actions that needs to be chosen.&lt;/p&gt;

&lt;p&gt;Over past few years, deep q learning has gained  lot of popularity. After the paper by Deep Mind about the Human level control sing reinforcement learning, there was no looking back. It combined the advanced in RL as well as deep learning to get an AI player which had superhuman performance. I made the basic &lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/dqn.jl&quot;&gt;DQN&lt;/a&gt; and &lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/double-dqn.jl&quot;&gt;Double DQN&lt;/a&gt; during the pre-GSoC phase, followed by &lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/duel-dqn.jl&quot;&gt;Duel DQN&lt;/a&gt; in the first week on GSoC.&lt;/p&gt;

&lt;p&gt;The idea used in the &lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/actor-critic/a2c.jl&quot;&gt;A2C model&lt;/a&gt; is different from the one in DQN. A2C falls in the class of “Actor-Critic” models. In AC models we have 2 neural networks, policy network and value network. policy network accepts the state of the game and returns a probability distribution over the action space. Value Nework takes the state and action chosen using policy network as input and determines how suitable is that action for that state.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/tree/master/ddpg&quot;&gt;DDPG&lt;/a&gt; is particularly useful when the actions which needs to be chosed are spread over a continuous space. one possible solution you may have in mind is that what if we discretize the action space? If we discretize it narrowly we end up with a large number of actions. If we discretize it sparsely then we lose important data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/ddpg.png&quot; alt=&quot;DDPG&quot; /&gt;
&lt;br /&gt;
                                                 DDPG: Score vs Episodes&lt;/p&gt;

&lt;p&gt;Some of these models have been deployed on Flux’s &lt;a href=&quot;https://fluxml.ai&quot;&gt;website&lt;/a&gt;. CartPole example has been trained on Deep Q Networks and the pong example is trained on Duel-DQN.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=L3pqMUDVrT0&quot;&gt;Here&lt;/a&gt; is a demo of Pong trained using Flux.&lt;/p&gt;
&lt;h4 id=&quot;targets-achieved&quot;&gt;Targets achieved&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/actor-critic/a2c.jl&quot;&gt;Advantage Actor-Critic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/duel-dqn.jl&quot;&gt;Duel DQN&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;extra-mile&quot;&gt;Extra mile&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/tree/master/ddpg&quot;&gt;DDPG&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/prioritized-replay-dqn.jl&quot;&gt;Prioritized DQN&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;future-work&quot;&gt;Future Work&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Add more variety of models, especially the ones which have come up in the last 18 momnths.&lt;/li&gt;
  &lt;li&gt;Create an interface to easily train and test any environment from &lt;a href=&quot;https://github.com.JuliaML/OpenAIGym.jl&quot;&gt;OpenAIGym.jl&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;2-alphagojl&quot;&gt;2. &lt;a href=&quot;https://github.com/tejank10/AlphaGo.jl&quot;&gt;AlphaGo.jl&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This mini-project of the GSoC phase 2 was the most challenging part. AlphaGo Zero is a breakthrough AI by Google DeepMind. It is an AI to play Go, which is considered to be one of most challeenging games in the world, mainly  due to number of states it can lead to. AlphaGo Zero defeated the best Go player in the world. AlphaFo.jl’s objective is achieve the results produced by AlphaGo Zero algorithm over Go, and achieve similar results on any zero-sum game.&lt;/p&gt;

&lt;p&gt;Now, we have a package to train AlphaGo zero model in Julia! And it is really simple to train the model. We just have to pass the training parameters, the environment on which we want to train the model and then play with it.
For more info in the AlphaGo.jl refer to the &lt;a href=&quot;https://tejank10.github.io/jekyll/update/2018/07/08/GSoC-Phase-2.html&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;targets-achieved-1&quot;&gt;Targets achieved&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Game of Go&lt;/li&gt;
  &lt;li&gt;Monte Carlo tree search&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;targets-couldnt-achieve&quot;&gt;Targets couldn’t achieve&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Couldn’t train the model well&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;extra-mile-1&quot;&gt;Extra Mile&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Game of Gomoku to test the algorithm (since it is easier game)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;future-work-1&quot;&gt;Future work&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Train a model on any game&lt;/li&gt;
  &lt;li&gt;AlphaChess&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;3-generative-adversarial-networks&quot;&gt;3. &lt;a href=&quot;https://github.com/tejank10/model-zoo/tree/GAN/vision/mnist&quot;&gt;Generative Adversarial Networks&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;GANs have been extremely suceessful in learning the underlying representation of any data. By doing so, it can reproduce some fake data. For example the GANs trained on MNIST Human handwritten digits dataset can produce some fake images which look very similar to those in the MNIST. These neural nets have great application in image editing. It can remove certain features from the image, add some new ones; depending on the dataset. The GANs contain of two networks: generator and discriminator. Generator’s objective os to generate fake images awhereas the discriminator’s objective is to differentiate between the fake images generted by thhe generator and the real images in the  dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/lsgan.gif&quot; alt=&quot;LSGAN&quot; width=&quot;170px&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/dcgan.gif&quot; alt=&quot;DCGAN&quot; width=&quot;170px&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/giphy.gif&quot; alt=&quot;WGAN&quot; width=&quot;170px&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/made.gif&quot; alt=&quot;MADE&quot; width=&quot;170px&quot; /&gt;
&lt;br /&gt;
               LSGAN                               DCGAN                               WGAN                               MADE&lt;/p&gt;

&lt;h4 id=&quot;targets-acheived&quot;&gt;Targets acheived&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/FluxML/Flux.jl/pull/311&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ConvTranspose&lt;/code&gt; layer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/model-zoo/blob/GAN/vision/mnist/dcgan.jl&quot;&gt;DCGAN&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;extra-mile-2&quot;&gt;Extra Mile&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/model-zoo/blob/GAN/vision/mnist/lsgan.jl&quot;&gt;LSGAN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/model-zoo/blob/GAN/vision/mnist/wgan.jl&quot;&gt;WGAN&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;future-work-2&quot;&gt;Future work&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;More models of GAN like infoGAN, BEGAN, CycleGAN&lt;/li&gt;
  &lt;li&gt;Some cool animations with GANs&lt;/li&gt;
  &lt;li&gt;Data pipeline for training and producing images with dataset, and GAN type as input.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;4-decoupled-neural-interface&quot;&gt;4. &lt;a href=&quot;https://github.com/tejank10/model-zoo/tree/DNI/vision/mnist/dni.jl&quot;&gt;Decoupled Neural Interface&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Decoupled Neural Interface is a new technique to train the momdel. It does not use the backpropagation from the output layer right upto the input layer. Instead it uses a trick to “estimate” the gradient. It has a small linear layer neural network to predict the gradients, instead of running the backpropagation rather than finding the true gradients. The advantage of such a model is that it can be parallelized. This technique results in slight dip in the accuracy, but we have improved speed if we have parallelized the layers in the network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/loss.png&quot; alt=&quot;loss&quot; width=&quot;362.5px&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/tejank10/tejank10.github.io/master/assets/acc.png&quot; alt=&quot;loss&quot; width=&quot;362.5px&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;targets-achieved-2&quot;&gt;Targets achieved:&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/model-zoo/tree/DNI/vision/mnist/dni.jl&quot;&gt;DNI model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;During the past three months, I learn a lot about Reinforcement Learning and AlphaGo in particular. I experienced training an RL model for days, finally saw it working well! I encountered the issues faced in training the models and learnt to overcome them. All in all, as an aspiring ML engineer these three months have been the most productive months. I am glad that I could meet most of my objectives. I have worked on some extra models to make up for the objectives I could not meet.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;I really would like to thank my mentor Mike Innes for guiding me throughtout the project, and James Bradbury for his valuable inputs for improving the code in the Reinforcement Learning models. I also would like to thank Neethu Mariya Joy for deploying the trained models on th web. And last but not the least, NumFOCUS: for sponsoring me and all other JSoC students for JuliaCon’18 London.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hello, world!</summary></entry><entry><title type="html">GSoC’18: AlphaGo.jl</title><link href="http://localhost:4000/jekyll/update/2018/07/08/GSoC-Phase-2.html" rel="alternate" type="text/html" title="GSoC'18: AlphaGo.jl" /><published>2018-07-08T16:00:00+05:30</published><updated>2018-07-08T16:00:00+05:30</updated><id>http://localhost:4000/jekyll/update/2018/07/08/GSoC-Phase-2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/07/08/GSoC-Phase-2.html">&lt;p&gt;Hello, world!&lt;/p&gt;

&lt;p&gt;The phase 2 of GSoC is over and &lt;a href=&quot;https://github.com/tejank10/AlphaGo.jl&quot;&gt;AlphaGo.jl&lt;/a&gt; is ready! In this post I am going to explain about the usage of it.
&lt;a href=&quot;https://github.com/tejank10/AlphaGo.jl&quot;&gt;AlphaGo.jl&lt;/a&gt; is built to try and test the Alpha(Go)Zero algorithm with your own parameters on the game of Go. Today, I’ll explain about higher level methods of it. For more details, which mainly includes MCTS implementation, you can check out the &lt;a href=&quot;https://github.com/tejank10/AlphaGo.jl&quot;&gt;repo&lt;/a&gt;. It is built using &lt;a href=&quot;https://www.fluxml.ai&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Flux.jl&lt;/code&gt;&lt;/a&gt;, a machine learning library for &lt;a href=&quot;https://www.julialang.org&quot;&gt;Julia&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;environment&quot;&gt;Environment&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GameEnv&lt;/code&gt; is an &lt;code class=&quot;highlighter-rouge&quot;&gt;abstract type&lt;/code&gt; used to represent game environment. Setting up environment is the first thing to do before starting with anything. This is because environment stores important information about the game which is used by other modules. Other modules require environment as input in order to set up themselves using this information.&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;
&lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GoEnv&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here we have set up environment for Go having board size of 9x9.&lt;/p&gt;

&lt;h3 id=&quot;neuralnet&quot;&gt;NeuralNet&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt; structure is used to store the AlphaZero neural network. The AlphaZero neural network is made up of three parts. A base network is there, which branches out into value netwrok and policy network.
The base network accepts Position of the board as input. Value network outputs a single value between -1 to 1 denoting who will win from the given position. -1 denotes white will win from that position and 1 implies black. The policy network returns the probability distribution over the different actions for that position of board.&lt;/p&gt;

&lt;p&gt;You can replace any of these three networks with your own Flux model, provided it is consistent with the whole pipeline fo the &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;mutable&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NeuralNet&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;base_net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chain&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chain&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chain&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt; NeuralNet&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nothing&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nothing&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nothing&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;tower_height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GameEnv&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nothing&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;res_block&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResidualBlock&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;])&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# 19 residual blocks&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;tower&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res_block&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tower_height&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;base_net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Chain&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;planes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BatchNorm&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;tower&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gpu&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nothing&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Chain&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BatchNorm&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)),&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gpu&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nothing&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Chain&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BatchNorm&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)),&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gpu&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;all_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vcat&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_net&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Momentum&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_params&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.02f0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base_net&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;mctsplayer&quot;&gt;MCTSPlayer&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt; struct simulates a game using Monte-Carlo Tree Search and &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt;. It takes &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;env&lt;/code&gt; as input. The player plays the game upto the number of readouts. &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt; can perform following functions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MCTS&lt;/li&gt;
  &lt;li&gt;Pick a move based on MCTS and play it&lt;/li&gt;
  &lt;li&gt;Extract data from the games played by it&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These functionalities are used during the training and testing phase.&lt;/p&gt;
&lt;h3 id=&quot;selfplay&quot;&gt;Selfplay&lt;/h3&gt;
&lt;p&gt;Self-play stage is used in the training phase. In this stage, the  &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt; plays a game against itself. Every move in the game is picked based on the MCTS and played. After the game ends, the &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt; object is returned for extraction of data.&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;train()&lt;/code&gt; method is used by the user to train the model based on the following parameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;env&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;num_games&lt;/code&gt;: Number of self-play games to be played
Optional arguments:&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;memory_size&lt;/code&gt;: Size of the memory buffer&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;epochs&lt;/code&gt;: Number of epochs to train the data on&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ckp_freq&lt;/code&gt;: Frequecy of saving the model and weights&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tower_height&lt;/code&gt;: AlphaGo Zero Architecture uses residual networks stacked together. This is called a tower of residual networks. &lt;code class=&quot;highlighter-rouge&quot;&gt;tower_height&lt;/code&gt; specifies how many residual blocks to be stacked.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;model&lt;/code&gt;: Object of type &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;readouts&lt;/code&gt;: number of readouts by &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;start_training_after&lt;/code&gt;: Number of games after which training will be started&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;train()&lt;/code&gt; starts off with a game of &lt;code class=&quot;highlighter-rouge&quot;&gt;selfplay()&lt;/code&gt; using the current best &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt;. On completion of the game, the data from that game  is extracted. This includes the board states, policy used at each move,and the result of that game. This data is stored in the memory buffer.&lt;/p&gt;
&lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_games&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;player&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;selfplay&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cur_nn&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readouts&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;extract_data&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;player&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;pos_buffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vcat&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;π_buffer&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vcat&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;π_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;res_buffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vcat&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;memory_size&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pos_buffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;π_buffer&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;π_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res_buffer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_training_after&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;replay_pos&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replay_π&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replay_res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_replay_batch&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;π_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res_buffer&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train!&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur_nn&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replay_pos&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replay_π&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replay_res&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;player&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_string&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_moves&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;player&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Episode &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;i over. Loss: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;loss. Winner: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;result. Moves: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;num_moves.&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ckp_freq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;save_model&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cur_nn&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Model saved. &quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;At every training step, &lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size&lt;/code&gt;number of samples are picked from the memory. The features are extracted from the board states picked and fed into the &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt;, which gives out the value and policy as described above in the &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt; section.&lt;/p&gt;

&lt;p&gt;We then compute losses. There are three kinds of losses here: Policy loss, Value loss and L2 regularisation.&lt;/p&gt;
&lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Policy loss: p is predicted policy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss_π&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;crossentropy&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;π&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01f0&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Value loss&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss_value&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01f0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The losses are added and backpropagated, after which the optimizer updates the weights. &lt;code class=&quot;highlighter-rouge&quot;&gt;epochs&lt;/code&gt; can be specified in the &lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt; call to train on this data. Periodically, the &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt; and its weights are backed up using &lt;a href=&quot;http://github.com/MikeInnes/BSON.jl&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;BSON.jl&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;play&quot;&gt;Play&lt;/h2&gt;
&lt;p&gt;To play against saved &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt; model, we have to load it using &lt;code class=&quot;highlighter-rouge&quot;&gt;load_model&lt;/code&gt;. It accepts path of the model and &lt;code class=&quot;highlighter-rouge&quot;&gt;env&lt;/code&gt; as parameters and returns an object of &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt;.
&lt;code class=&quot;highlighter-rouge&quot;&gt;play()&lt;/code&gt; takes following arguments:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;env&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn&lt;/code&gt;: an object of type &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tower_height&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;num_readouts&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mode&lt;/code&gt;: It specifies human will play as Black or white. If &lt;code class=&quot;highlighter-rouge&quot;&gt;mode&lt;/code&gt; is 0 then human is Black, else White.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sample-usage&quot;&gt;Sample usage&lt;/h3&gt;
&lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AlphaGo&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# This makes a Go board of 9x9&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GoEnv&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# A NeuralNet object of tower_height 10 is made and trained and returned&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;neural_net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_games&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ckp_freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tower_height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_training_after&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Plays a game against the trained network, with human as White&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;play&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neural_net&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">Hello, world!</summary></entry><entry><title type="html">GSoC’18: From Go to AlphaGo</title><link href="http://localhost:4000/jekyll/update/2018/06/09/GSoC-week3-4.html" rel="alternate" type="text/html" title="GSoC'18: From Go to AlphaGo" /><published>2018-06-09T16:00:00+05:30</published><updated>2018-06-09T16:00:00+05:30</updated><id>http://localhost:4000/jekyll/update/2018/06/09/GSoC-week3-4</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/06/09/GSoC-week3-4.html">&lt;p&gt;Hello, world!&lt;/p&gt;

&lt;p&gt;In the last post, I had talked about the game of Go and its dynamics. In last two weeks, I worked on the set objectives which were Monte-Carlo Tree Search and putting things together to make AlphaGo Zero.&lt;/p&gt;

&lt;p&gt;In the week 3, it was mostly about MCTS. The MCTS is organized into two parts. One is a &lt;code class=&quot;highlighter-rouge&quot;&gt;struct&lt;/code&gt; for a node of tree. The other is a &lt;code class=&quot;highlighter-rouge&quot;&gt;struct&lt;/code&gt; for Player which uses MCTS to perform move. &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt; is the &lt;code class=&quot;highlighter-rouge&quot;&gt;struct&lt;/code&gt; with which the user will interact. A node of Monte-Carlo tree is defined by a board position, and the different positions it can go to upon playing any of the moves from that board position in the action space. &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt; provides an API to perform Tree Search. It then selects a move based on tree search, and perform it. While performing tree search virtual loss was used. It means that when selecting a node, it is pretended that evaluation has already taken place. This introduces some stochasticity in selection of node.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt; consists of a neural network. This neural network is used to generate a prediction of policy and value for a given input of board position. This prediction is used to update the values related to the nodes (which we had earlier used as virtual loss). &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt;’s neural network is of type &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt;, which is broken down into 3 parts: Base network, value head and policy head. The input passes through the base network first. The output of it is fed into value head to obtain value of state and into policy head to obtain the policy for it. &lt;code class=&quot;highlighter-rouge&quot;&gt;NeuralNet&lt;/code&gt; can also perform the evaluation of two &lt;code class=&quot;highlighter-rouge&quot;&gt;MCTSPlayer&lt;/code&gt;s, where two players compete in a series of games to decide who is the winner.&lt;/p&gt;

&lt;p&gt;I have put up an example of AlphaGo Zero algorithm &lt;a href=&quot;https://github.com/tejank10/AlphaGo.jl/blob/master/src/play.jl&quot;&gt;here&lt;/a&gt;. By setting the flags, you can run it. By default it runs the default AGZ algorithms from the paper.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hello, world!</summary></entry><entry><title type="html">GSoC’18: Flux baselines, Go and more</title><link href="http://localhost:4000/jekyll/update/2018/05/26/GSoC-week1-2.html" rel="alternate" type="text/html" title="GSoC'18: Flux baselines, Go and more" /><published>2018-05-26T16:00:00+05:30</published><updated>2018-05-26T16:00:00+05:30</updated><id>http://localhost:4000/jekyll/update/2018/05/26/GSoC-week1-2</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/26/GSoC-week1-2.html">&lt;p&gt;Hello, world!&lt;/p&gt;

&lt;p&gt;The commmunity bonding period and first 2 weeks of GSoC has come to an end. Community bonding period lasted over three weeks. I coult not do much work over first two weeks due to my end semester exams. In the third week, I implemented &lt;a href=&quot;https://arxiv.org/abs/1502.03509&quot;&gt;MADE&lt;/a&gt;, which is Masked Autoencoder for Distributed Estimation.  I also added dilation feature for convolutions (which was a &lt;a href=&quot;https://github.com/FluxML/NNlib.jl/pull/31#issuecomment-386673632&quot;&gt;feature request&lt;/a&gt; for NNlib.jl).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/FluxML/model-zoo/pull/39&quot;&gt;PR#19&lt;/a&gt;: Implemented MADE architecture in Flux.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/FluxML/NNlib.jl/pull/40&quot;&gt;PR#40&lt;/a&gt;: Dilation support for convolutions. So far, dilation support is available for 1D, 2D and 3D convolutions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In week 1, I implemented and created demos of some seminal papers in deep reinforcement learning. The algorithms implemented were tested on environments in OpenAI Gym using the package &lt;a href=&quot;https://github.com/JuliaML/OpenAIGym.jl&quot;&gt;OpenAIGym.jl&lt;/a&gt;. Environments used for testing are CartPole-v0, Pong-v0 and Pendulum-v0. The work done in this regard over pre-GSoC period and this week has been compiled into &lt;a href=&quot;https://github.com/tejank10/Flux-baselines&quot;&gt;Flux baselines&lt;/a&gt; repo. As of now it contains 6 models, which include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/dqn.jl&quot;&gt;Deep Q Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/double-dqn.jl&quot;&gt;Double DQN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/prioritized-replay-dqn.jl&quot;&gt;Prioritized Experience Replay DQN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/dqn/duel-dqn.jl&quot;&gt;Dueling DDQN &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/actor-critic/a2c.jl&quot;&gt;Advantage Actor-Critic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tejank10/Flux-baselines/blob/master/ddpg/ddpg.jl&quot;&gt;Deep Deterministic Policy Gradients&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In week 2, I started working towards one major milestone of the project: the &lt;a href=&quot;https://deepmind.com/blog/alphago-zero-learning-scratch/&quot;&gt;AlphaGo Zero&lt;/a&gt; model. For those of you not familiar, AlphaGo Zero is latest version of AlphaGo which is a program to play (and defeat :P) the ancient Chinese game of Go. This version of AlphaGo doesn’t take any human amateur and professional games to learn how to play Go. Instead it learns to play by playing games against itself, starting from completely random play.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tejank10/AlphaGo.jl&quot;&gt;AlphaGo.jl&lt;/a&gt; is where I am implementing the Flux based version of AlphaGo Zero. This part of project is divided  into three tasks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Creating the environment for Go&lt;/li&gt;
  &lt;li&gt;Monte-Carlo Tree Search&lt;/li&gt;
  &lt;li&gt;Main model of AlphaGo Zero using Go and MCTS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In week 2 I created the environment of Go. The environment simulates the game of Go, with abstraction like that of OpenAI Gym. The game can be played on a board of size 9x9, 13x13, 17x17 or 19x19. A player is assigned stones of either black or white color. The player with black stones makes the first move. Now, this can be an advantage for the black player. Hence white player is awarded some extra points. These extra points are called komi. Players can place a stone on any intersection of a vertical and horizontal lines on the board. A &lt;em&gt;NxN&lt;/em&gt; board has &lt;em&gt;N^2&lt;/em&gt; intersections.  On a player’s turn, he can either place a stone or can pass to the other player. Thus, action space for the environment is &lt;em&gt;N^2 + 1&lt;/em&gt;. The game ends when both the players pass consecutively. Depending on the end game state of the board, scores are calculated and winner is decided.&lt;/p&gt;

&lt;p&gt;In the coming days, my goals will be to complete the other two tasks. Hopefully in the next blog post, I’ll present the demo of the game before you.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hello, world!</summary></entry><entry><title type="html">Accepted to GSoC’18</title><link href="http://localhost:4000/jekyll/update/2018/05/04/Accepted-to-GSoC.html" rel="alternate" type="text/html" title="Accepted to GSoC'18" /><published>2018-05-04T16:00:00+05:30</published><updated>2018-05-04T16:00:00+05:30</updated><id>http://localhost:4000/jekyll/update/2018/05/04/Accepted-to-GSoC</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/04/Accepted-to-GSoC.html">&lt;p&gt;Hello, world!&lt;/p&gt;

&lt;p&gt;I am Tejan Karmali, a CS undergrad at NIT Goa. My proposal for GSoC’18 has been &lt;a href=&quot;https://summerofcode.withgoogle.com/projects/#5885584111828992&quot;&gt;accepted&lt;/a&gt; under NumFOCUS: Julia. I’ll be working on enriching the model zoo of &lt;a href=&quot;http://fluxml.ai/&quot;&gt;Flux.jl&lt;/a&gt;. Flux is a Machine Learning library written fully in Julia. My project aims at adding a variety of models in Flux. I’ll be adding Dueling DQN, Actor-Critic model, AlphaGo, DCGAN, Decoupling Neural Interface and Spatial Transformer Networks in the model zoo over the course of summer. These models are targeted towards the new users of Flux and to help them get started.&lt;/p&gt;

&lt;p&gt;My mentors are &lt;a href=&quot;https://mikeinnes.github.io/&quot;&gt;Mike Innes&lt;/a&gt;, &lt;a href=&quot;https://simondanisch.jimdo.com/&quot;&gt;Simon Danisch&lt;/a&gt;, &lt;a href=&quot;http://chrisrackauckas.com/&quot;&gt;Chris Rackauckas&lt;/a&gt; and &lt;a href=&quot;http://karpinski.org/&quot;&gt;Stefan Karpinski&lt;/a&gt;. I’ll be posting biweekly updates here about my project. Looking forward to an eventful and productive summer!&lt;/p&gt;</content><author><name></name></author><summary type="html">Hello, world!</summary></entry></feed>