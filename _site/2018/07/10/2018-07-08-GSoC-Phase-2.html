<p>Hello, world!</p>

<p>The phase 2 of GSoC is over and <a href="https://github.com/tejank10/AlphaGo.jl">AlphaGo.jl</a> is ready! In this post I am going to explain about the usage of it.
<a href="https://github.com/tejank10/AlphaGo.jl">AlphaGo.jl</a> is built to try and test the Alpha(Go)Zero algorithm with your own parameters on the game of Go. Today, I’ll explain about higher level methods of it. For more details, which mainly includes MCTS implementation, you can check out the <a href="https://github.com/tejank10/AlphaGo.jl">repo</a>. It is built using <a href="https://www.fluxml.ai"><code class="highlighter-rouge">Flux.jl</code></a>, a machine learning library for <a href="https://www.julialang.org">Julia</a>.</p>

<h3 id="environment">Environment</h3>
<p><code class="highlighter-rouge">GameEnv</code> is an <code class="highlighter-rouge">abstract type</code> used to represent game environment. Setting up environment is the first thing to do before starting with anything. This is because environment stores important information about the game which is used by other modules. Other modules require environment as input in order to set up themselves using this information.</p>

<p>Example:</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="n">GoEnv</span><span class="x">(</span><span class="mi">9</span><span class="x">)</span>
</code></pre></div></div>
<p>Here we have set up environment for Go having board size of 9x9.</p>

<h3 id="neuralnet">NeuralNet</h3>
<p><code class="highlighter-rouge">NeuralNet</code> structure is used to store the AlphaZero neural network. The AlphaZero neural network is made up of three parts. A base network is there, which branches out into value netwrok and policy network.
The base network accepts Position of the board as input. Value network outputs a single value between -1 to 1 denoting who will win from the given position. -1 denotes white will win from that position and 1 implies black. The policy network returns the probability distribution over the different actions for that position of board.</p>

<p>You can replace any of these three networks with your own Flux model, provided it is consistent with the whole pipeline fo the <code class="highlighter-rouge">NeuralNet</code>.</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mutable</span> <span class="n">struct</span> <span class="n">NeuralNet</span>
  <span class="n">base_net</span><span class="o">::</span><span class="n">Chain</span>
  <span class="n">value</span><span class="o">::</span><span class="n">Chain</span>
  <span class="n">policy</span><span class="o">::</span><span class="n">Chain</span>
  <span class="n">opt</span>

  <span class="k">function</span><span class="nf"> NeuralNet</span><span class="x">(</span><span class="n">env</span><span class="o">::</span><span class="n">T</span><span class="x">;</span> <span class="n">base_net</span> <span class="o">=</span> <span class="n">nothing</span><span class="x">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">nothing</span><span class="x">,</span> <span class="n">policy</span> <span class="o">=</span> <span class="n">nothing</span><span class="x">,</span>
                          <span class="n">tower_height</span><span class="o">::</span><span class="kt">Int</span> <span class="o">=</span> <span class="mi">19</span><span class="x">)</span> <span class="n">where</span> <span class="n">T</span> <span class="o">&lt;:</span> <span class="n">GameEnv</span>
    <span class="k">if</span> <span class="n">base_net</span> <span class="o">==</span> <span class="n">nothing</span>
      <span class="n">res_block</span><span class="x">()</span> <span class="o">=</span> <span class="n">ResidualBlock</span><span class="x">([</span><span class="mi">256</span><span class="x">,</span><span class="mi">256</span><span class="x">,</span><span class="mi">256</span><span class="x">],</span> <span class="x">[</span><span class="mi">3</span><span class="x">,</span><span class="mi">3</span><span class="x">],</span> <span class="x">[</span><span class="mi">1</span><span class="x">,</span><span class="mi">1</span><span class="x">],</span> <span class="x">[</span><span class="mi">1</span><span class="x">,</span><span class="mi">1</span><span class="x">])</span>
      <span class="c"># 19 residual blocks</span>
      <span class="n">tower</span> <span class="o">=</span> <span class="x">[</span><span class="n">res_block</span><span class="x">()</span> <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="x">:</span><span class="n">tower_height</span><span class="x">]</span>
      <span class="n">base_net</span> <span class="o">=</span> <span class="n">Chain</span><span class="x">(</span><span class="n">Conv</span><span class="x">((</span><span class="mi">3</span><span class="x">,</span><span class="mi">3</span><span class="x">),</span> <span class="n">env</span><span class="o">.</span><span class="n">planes</span><span class="o">=&gt;</span><span class="mi">256</span><span class="x">,</span> <span class="n">pad</span><span class="o">=</span><span class="x">(</span><span class="mi">1</span><span class="x">,</span><span class="mi">1</span><span class="x">)),</span> <span class="n">BatchNorm</span><span class="x">(</span><span class="mi">256</span><span class="x">,</span> <span class="n">relu</span><span class="x">),</span>
                        <span class="n">tower</span><span class="o">...</span><span class="x">)</span> <span class="o">|&gt;</span> <span class="n">gpu</span>
    <span class="k">end</span>
    <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="n">nothing</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">Chain</span><span class="x">(</span><span class="n">Conv</span><span class="x">((</span><span class="mi">1</span><span class="x">,</span><span class="mi">1</span><span class="x">),</span> <span class="mi">256</span><span class="o">=&gt;</span><span class="mi">1</span><span class="x">),</span> <span class="n">BatchNorm</span><span class="x">(</span><span class="mi">1</span><span class="x">,</span> <span class="n">relu</span><span class="x">),</span> <span class="n">x</span><span class="o">-&gt;</span><span class="n">reshape</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="x">:,</span> <span class="n">size</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="mi">4</span><span class="x">)),</span>
                    <span class="n">Dense</span><span class="x">(</span><span class="n">env</span><span class="o">.</span><span class="n">N</span><span class="o">*</span><span class="n">env</span><span class="o">.</span><span class="n">N</span><span class="x">,</span> <span class="mi">256</span><span class="x">,</span> <span class="n">relu</span><span class="x">),</span> <span class="n">Dense</span><span class="x">(</span><span class="mi">256</span><span class="x">,</span> <span class="mi">1</span><span class="x">,</span> <span class="n">tanh</span><span class="x">))</span> <span class="o">|&gt;</span> <span class="n">gpu</span>
    <span class="k">end</span>
    <span class="k">if</span> <span class="n">policy</span> <span class="o">==</span> <span class="n">nothing</span>
      <span class="n">policy</span> <span class="o">=</span> <span class="n">Chain</span><span class="x">(</span><span class="n">Conv</span><span class="x">((</span><span class="mi">1</span><span class="x">,</span><span class="mi">1</span><span class="x">),</span> <span class="mi">256</span><span class="o">=&gt;</span><span class="mi">2</span><span class="x">),</span> <span class="n">BatchNorm</span><span class="x">(</span><span class="mi">2</span><span class="x">,</span> <span class="n">relu</span><span class="x">),</span> <span class="n">x</span><span class="o">-&gt;</span><span class="n">reshape</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="x">:,</span> <span class="n">size</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="mi">4</span><span class="x">)),</span>
                      <span class="n">Dense</span><span class="x">(</span><span class="mi">2</span><span class="n">env</span><span class="o">.</span><span class="n">N</span><span class="o">*</span><span class="n">env</span><span class="o">.</span><span class="n">N</span><span class="x">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="x">),</span> <span class="n">x</span> <span class="o">-&gt;</span> <span class="n">softmax</span><span class="x">(</span><span class="n">x</span><span class="x">))</span> <span class="o">|&gt;</span> <span class="n">gpu</span>
    <span class="k">end</span>

    <span class="n">all_params</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">params</span><span class="x">(</span><span class="n">base_net</span><span class="x">),</span> <span class="n">params</span><span class="x">(</span><span class="n">value</span><span class="x">),</span> <span class="n">params</span><span class="x">(</span><span class="n">policy</span><span class="x">))</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Momentum</span><span class="x">(</span><span class="n">all_params</span><span class="x">,</span> <span class="mf">0.02f0</span><span class="x">)</span>
    <span class="nb">new</span><span class="x">(</span><span class="n">base_net</span><span class="x">,</span> <span class="n">value</span><span class="x">,</span> <span class="n">policy</span><span class="x">,</span> <span class="n">opt</span><span class="x">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>
<h3 id="mctsplayer">MCTSPlayer</h3>
<p><code class="highlighter-rouge">MCTSPlayer</code> struct simulates a game using Monte-Carlo Tree Search and <code class="highlighter-rouge">NeuralNet</code>. It takes <code class="highlighter-rouge">NeuralNet</code> and <code class="highlighter-rouge">env</code> as input. The player plays the game upto the number of readouts. <code class="highlighter-rouge">MCTSPlayer</code> can perform following functions:</p>
<ul>
  <li>MCTS</li>
  <li>Pick a move based on MCTS and play it</li>
  <li>Extract data from the games played by it</li>
</ul>

<p>These functionalities are used during the training and testing phase.</p>
<h3 id="selfplay">Selfplay</h3>
<p>Self-play stage is used in the training phase. In this stage, the  <code class="highlighter-rouge">MCTSPlayer</code> plays a game against itself. Every move in the game is picked based on the MCTS and played. After the game ends, the <code class="highlighter-rouge">MCTSPlayer</code> object is returned for extraction of data.</p>

<h3 id="training">Training</h3>
<p><code class="highlighter-rouge">train()</code> method is used by the user to train the model based on the following parameters:</p>
<ul>
  <li><code class="highlighter-rouge">env</code></li>
  <li><code class="highlighter-rouge">num_games</code>: Number of self-play games to be played
Optional arguments:</li>
  <li><code class="highlighter-rouge">memory_size</code>: Size of the memory buffer</li>
  <li><code class="highlighter-rouge">batch_size</code></li>
  <li><code class="highlighter-rouge">epochs</code>: Number of epochs to train the data on</li>
  <li><code class="highlighter-rouge">ckp_freq</code>: Frequecy of saving the model and weights</li>
  <li><code class="highlighter-rouge">tower_height</code>: AlphaGo Zero Architecture uses residual networks stacked together. This is called a tower of residual networks. <code class="highlighter-rouge">tower_height</code> specifies how many residual blocks to be stacked.</li>
  <li><code class="highlighter-rouge">model</code>: Object of type <code class="highlighter-rouge">NeuralNet</code></li>
  <li><code class="highlighter-rouge">readouts</code>: number of readouts by <code class="highlighter-rouge">MCTSPlayer</code></li>
  <li><code class="highlighter-rouge">start_training_after</code>: Number of games after which training will be started</li>
</ul>

<p><code class="highlighter-rouge">train()</code> starts off with a game of <code class="highlighter-rouge">selfplay()</code> using the current best <code class="highlighter-rouge">NeuralNet</code>. On completion of the game, the data from that game  is extracted. This includes the board states, policy used at each move,and the result of that game. This data is stored in the memory buffer.</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="x">:</span><span class="n">num_games</span>
  <span class="n">player</span> <span class="o">=</span> <span class="n">selfplay</span><span class="x">(</span><span class="n">env</span><span class="x">,</span> <span class="n">cur_nn</span><span class="x">,</span> <span class="n">readouts</span><span class="x">)</span>
  <span class="n">p</span><span class="x">,</span> <span class="n">π</span><span class="x">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">extract_data</span><span class="x">(</span><span class="n">player</span><span class="x">)</span>

  <span class="n">pos_buffer</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">pos_buffer</span><span class="x">,</span> <span class="n">p</span><span class="x">)</span>
  <span class="n">π_buffer</span>   <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">π_buffer</span><span class="x">,</span> <span class="n">π</span><span class="x">)</span>
  <span class="n">res_buffer</span> <span class="o">=</span> <span class="n">vcat</span><span class="x">(</span><span class="n">res_buffer</span><span class="x">,</span> <span class="n">v</span><span class="x">)</span>

  <span class="k">if</span> <span class="n">length</span><span class="x">(</span><span class="n">pos_buffer</span><span class="x">)</span> <span class="o">&gt;</span> <span class="n">memory_size</span>
    <span class="n">pos_buffer</span> <span class="o">=</span> <span class="n">pos_buffer</span><span class="x">[</span><span class="k">end</span><span class="o">-</span><span class="n">memory_size</span><span class="o">+</span><span class="mi">1</span><span class="x">:</span><span class="k">end</span><span class="x">]</span>
    <span class="n">π_buffer</span>   <span class="o">=</span> <span class="n">π_buffer</span><span class="x">[</span><span class="k">end</span><span class="o">-</span><span class="n">memory_size</span><span class="o">+</span><span class="mi">1</span><span class="x">:</span><span class="k">end</span><span class="x">]</span>
    <span class="n">res_buffer</span> <span class="o">=</span> <span class="n">res_buffer</span><span class="x">[</span><span class="k">end</span><span class="o">-</span><span class="n">memory_size</span><span class="o">+</span><span class="mi">1</span><span class="x">:</span><span class="k">end</span><span class="x">]</span>
  <span class="k">end</span>

  <span class="k">if</span> <span class="n">length</span><span class="x">(</span><span class="n">pos_buffer</span><span class="x">)</span> <span class="o">&gt;=</span> <span class="n">start_training_after</span>
    <span class="n">replay_pos</span><span class="x">,</span> <span class="n">replay_π</span><span class="x">,</span> <span class="n">replay_res</span> <span class="o">=</span> <span class="n">get_replay_batch</span><span class="x">(</span><span class="n">pos_buffer</span><span class="x">,</span> <span class="n">π_buffer</span><span class="x">,</span> <span class="n">res_buffer</span><span class="x">;</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="x">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">train!</span><span class="x">(</span><span class="n">cur_nn</span><span class="x">,</span> <span class="x">(</span><span class="n">replay_pos</span><span class="x">,</span> <span class="n">replay_π</span><span class="x">,</span> <span class="n">replay_res</span><span class="x">);</span> <span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span><span class="x">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">player</span><span class="o">.</span><span class="n">result_string</span>
    <span class="n">num_moves</span> <span class="o">=</span> <span class="n">player</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">position</span><span class="o">.</span><span class="n">n</span>
    <span class="n">println</span><span class="x">(</span><span class="s">"Episode </span><span class="si">$</span><span class="s">i over. Loss: </span><span class="si">$</span><span class="s">loss. Winner: </span><span class="si">$</span><span class="s">result. Moves: </span><span class="si">$</span><span class="s">num_moves."</span><span class="x">)</span>
  <span class="k">end</span>

  <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">ckp_freq</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">save_model</span><span class="x">(</span><span class="n">cur_nn</span><span class="x">)</span>
    <span class="n">print</span><span class="x">(</span><span class="s">"Model saved. "</span><span class="x">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>
<p>At every training step, <code class="highlighter-rouge">batch_size</code>number of samples are picked from the memory. The features are extracted from the board states picked and fed into the <code class="highlighter-rouge">NeuralNet</code>, which gives out the value and policy as described above in the <code class="highlighter-rouge">NeuralNet</code> section.</p>

<p>We then compute losses. There are three kinds of losses here: Policy loss, Value loss and L2 regularisation.</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Policy loss: p is predicted policy</span>
<span class="n">loss_π</span><span class="x">(</span><span class="n">π</span><span class="x">,</span> <span class="n">p</span><span class="x">)</span> <span class="o">=</span> <span class="n">crossentropy</span><span class="x">(</span><span class="n">p</span><span class="x">,</span> <span class="n">π</span><span class="x">;</span> <span class="n">weight</span> <span class="o">=</span> <span class="mf">0.01f0</span><span class="x">)</span>

<span class="c"># Value loss</span>
<span class="n">loss_value</span><span class="x">(</span><span class="n">z</span><span class="x">,</span> <span class="n">v</span><span class="x">)</span> <span class="o">=</span> <span class="mf">0.01f0</span> <span class="o">*</span> <span class="n">mse</span><span class="x">(</span><span class="n">z</span><span class="x">,</span> <span class="n">v</span><span class="x">)</span>
</code></pre></div></div>

<p>The losses are added and backpropagated, after which the optimizer updates the weights. <code class="highlighter-rouge">epochs</code> can be specified in the <code class="highlighter-rouge">train</code> call to train on this data. Periodically, the <code class="highlighter-rouge">NeuralNet</code> and its weights are backed up using <a href="http://github.com/MikeInnes/BSON.jl"><code class="highlighter-rouge">BSON.jl</code></a>.</p>

<h2 id="play">Play</h2>
<p>To play against saved <code class="highlighter-rouge">NeuralNet</code> model, we have to load it using <code class="highlighter-rouge">load_model</code>. It accepts path of the model and <code class="highlighter-rouge">env</code> as parameters and returns an object of <code class="highlighter-rouge">NeuralNet</code>.
<code class="highlighter-rouge">play()</code> takes following arguments:</p>
<ul>
  <li><code class="highlighter-rouge">env</code></li>
  <li><code class="highlighter-rouge">nn</code>: an object of type <code class="highlighter-rouge">NeuralNet</code></li>
  <li><code class="highlighter-rouge">tower_height</code></li>
  <li><code class="highlighter-rouge">num_readouts</code></li>
  <li><code class="highlighter-rouge">mode</code>: It specifies human will play as Black or white. If <code class="highlighter-rouge">mode</code> is 0 then human is Black, else White.</li>
</ul>

<h3 id="sample-usage">Sample usage</h3>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">using</span> <span class="n">AlphaGo</span>

<span class="c"># This makes a Go board of 9x9</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">GoEnv</span><span class="x">(</span><span class="mi">9</span><span class="x">)</span>

<span class="c"># A NeuralNet object of tower_height 10 is made and trained and returned</span>
<span class="n">neural_net</span> <span class="o">=</span> <span class="n">train</span><span class="x">(</span><span class="n">env</span><span class="x">,</span> <span class="n">num_games</span><span class="o">=</span><span class="mi">100</span><span class="x">,</span> <span class="n">ckp_freq</span><span class="o">=</span><span class="mi">10</span><span class="x">,</span> <span class="n">tower_height</span><span class="o">=</span><span class="mi">10</span><span class="x">,</span> <span class="n">start_training_after</span><span class="o">=</span><span class="mi">500</span><span class="x">)</span>

<span class="c"># Plays a game against the trained network, with human as White</span>
<span class="n">play</span><span class="x">(</span><span class="n">env</span><span class="x">,</span> <span class="n">neural_net</span><span class="x">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="mi">1</span><span class="x">)</span>
</code></pre></div></div>
